{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Illustrating KKLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import sys\n",
    "import pandas as pd\n",
    "import random\n",
    "import csv\n",
    "import datetime\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to generate the data (X,Y).  \n",
    "X  $\\in \\mathbb{R}^{D}$ and Y $\\in$ $\\mathbb{R}^{D}$. (X,Y) are jointly Gaussian with correlation between each entry $X_i$ and $Y_i$ as $\\rho$   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_xy_corr(rho, total_data_size, mean, cov, seed):\n",
    "    np.random.seed(seed)\n",
    "    return np.random.multivariate_normal(mean, cov, total_data_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function to store the data in tensors and also output a shuffled tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_generation_mi(rho, total_data_size, mean, cov, D, seed):    \n",
    "    Data = gen_xy_corr(rho, total_data_size, mean, cov, seed).T\n",
    "    data_size = np.int(total_data_size/D)\n",
    "    Data_xy  = np.array([[[0.0,0.0] for i in range(D)] for j in range(data_size)])\n",
    "    Data_y1 = np.array([[0.0 for i in range(D)] for j in range(data_size)])\n",
    "    Data_x1 = np.array([[0.0 for i in range(D)] for j in range(data_size)])\n",
    "    \n",
    "    for i in range(data_size):\n",
    "        Data_xy[i] = Data[:,(i)*D:((i+1)*D)].T\n",
    "        Data_y1[i] = Data_xy[i].T[1] \n",
    "        Data_x1[i] = Data_xy[i].T[0]\n",
    "    \n",
    "    Data_x1_tensor = tf.constant(Data_x1, tf.float32)\n",
    "    Data_y1_tensor = tf.constant(Data_y1, tf.float32)   \n",
    "    Data_y1_shuffle = tf.random_shuffle(Data_y1_tensor)\n",
    "    Data_y1_shuffle_tensor =  tf.constant(Data_y1_shuffle) \n",
    "    Data_xy1_tensor = tf.concat([Data_x1_tensor, Data_y1_tensor], axis=1)  \n",
    "    Data_xy1_shuffle_tensor = tf.concat([Data_x1_tensor, Data_y1_shuffle_tensor], axis=1)  \n",
    "    Data_xy1_tensor = tf.cast(Data_xy1_tensor, tf.float32)\n",
    "    Data_xy1_shuffle_tensor = tf.cast(Data_xy1_shuffle_tensor, tf.float32)  \n",
    "    return  Data_xy1_tensor, Data_xy1_shuffle_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing MINE for D=1 and comparing RMSE, Bias, Variance, across different correlations for large dataset\n",
    "\n",
    "Use MINE to compute mutual information between X and Y and compare to the true mutual information for different rhos. The comparison is done in terms of Bias and RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/Work_ML/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:642: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Total time for this block is 1666.2980301380157 seconds\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_random_seed(1000)\n",
    "start = time.time()\n",
    "n_epochs= 500\n",
    "n_sub_epochs = 1\n",
    "l1=0.0\n",
    "l2=0.0\n",
    "total_data_size = 1000\n",
    "D_vector = [2]\n",
    "rho_vector = [0.2, 0.5, 0.9]\n",
    "\n",
    "M= len(rho_vector)\n",
    "\n",
    "N_trials = 100\n",
    "MI_vector = np.array([0.0]*M)\n",
    "MI_vector_estimated  = np.array([[0.0]*N_trials]*M)\n",
    "Bias      = np.array([[0.0]*N_trials]*M)\n",
    "Bias_mean = np.array([0.0]*M)\n",
    "SE      = np.array([[0.0]*N_trials]*M)\n",
    "RMSE_mean = np.array([0.0]*M)\n",
    "Variance = np.array([0.0]*M)\n",
    "z=0\n",
    "for j in range(M):\n",
    "\n",
    "    rho            = rho_vector[j]\n",
    "    mean           = np.array([0.0, 0.0])\n",
    "    cov            = np.array([[1, rho], [rho, 1]])\n",
    "    D              = 1\n",
    "    MI_vector[j]   = -D/2*np.log(1-rho**2)\n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "    for w in range(N_trials):  \n",
    "        \n",
    "        optimizer      = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "        loss_history   = []\n",
    "        MIs            = []\n",
    "        model_mlp = tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(25,input_shape=[None,2*D], activation='relu'),\n",
    "          tf.keras.layers.Dense(1)\n",
    "        ])  \n",
    "    \n",
    "        seed=0\n",
    "        for epoch in range(n_epochs):\n",
    "            seed=seed+1\n",
    "            Data_xy1_tensor, Data_xy1_shuffle_tensor = Data_generation_mi(rho, total_data_size, mean, cov, D, seed)\n",
    "            for sub_epoch in range(n_sub_epochs):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    T_xy = model_mlp(Data_xy1_tensor)\n",
    "                    T_x_y = model_mlp(Data_xy1_shuffle_tensor)\n",
    "                    loss_value = -(tf.reduce_mean(T_xy, axis=0) - tf.math.log(tf.reduce_mean(tf.math.exp(T_x_y)))) + l1*(tf.abs(tf.reduce_mean(T_xy, axis=0)))+l2*(tf.abs(tf.math.log(tf.reduce_mean(tf.math.exp(T_x_y), axis=0))))\n",
    "                \n",
    "                loss_history.append(loss_value.numpy())\n",
    "                grads = tape.gradient(loss_value, model_mlp.variables)\n",
    "                    \n",
    "                optimizer.apply_gradients(zip(grads, model_mlp.variables)) \n",
    "                MIs.append((tf.reduce_mean(T_xy, axis=0) - tf.math.log(tf.reduce_mean(tf.math.exp(T_x_y)))))\n",
    "        Nl = len(MIs)\n",
    "        MI_vector_estimated[j][w] = MIs[Nl-1].numpy()[0]\n",
    "        Bias[j][w]                = MIs[Nl-1].numpy()[0] -MI_vector[j]\n",
    "        SE[j][w]                  = (Bias[j][w])**2 \n",
    "    Bias_mean[j]                  = np.mean(Bias[j])\n",
    "    RMSE_mean[j]                  = np.sqrt(np.mean(SE[j]))\n",
    "    Variance[j]                   = np.var(MI_vector_estimated[j])\n",
    "end = time.time()\n",
    "print (\"Total time for this block is \" + str(end-start) + \" seconds\") \n",
    "DataFrame_results1 = pd.DataFrame(np.array((Bias_mean, RMSE_mean,Variance, rho_vector, MI_vector)).T,  columns = ['Bias', 'RMSE', 'Variance','Correlation', 'True Mutual Information'])    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Bias      RMSE  Variance  Correlation  True Mutual Information\n",
      "0 -0.009427  0.011288  0.000039          0.2                 0.020411\n",
      "1 -0.025266  0.030608  0.000299          0.5                 0.143841\n",
      "2 -0.060437  0.075191  0.002001          0.9                 0.830366\n"
     ]
    }
   ],
   "source": [
    "print (DataFrame_results1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing KKLE for D=1 and comparing RMSE, Bias, Variance, across different correlations for large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time for this block is 10317.603276252747 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "n_epochs= 500\n",
    "n_sub_epochs = 1\n",
    "l1=0.0\n",
    "l2=0.0\n",
    "total_data_size = 1000\n",
    "D_vector = [2]\n",
    "rho_vector = [0.2, 0.5, 0.9]\n",
    "M = len(rho_vector)\n",
    "N_trials = 100\n",
    "\n",
    "Map_D   = 500\n",
    "MI_vector = np.array([0.0]*M)\n",
    "MI_vector_estimated1  = np.array([[0.0]*N_trials]*M)\n",
    "Bias1      = np.array([[0.0]*N_trials]*M)\n",
    "Bias_mean1 = np.array([0.0]*M)\n",
    "SE1      = np.array([[0.0]*N_trials]*M)\n",
    "RMSE_mean1 = np.array([0.0]*M)\n",
    "Variance1 = np.array([0.0]*M)\n",
    "\n",
    "for j in range(M):\n",
    "    rho            = rho_vector[j]\n",
    "    mean           = np.array([0.0, 0.0])\n",
    "    cov            = np.array([[1, rho], [rho, 1]])\n",
    "    D              = 1\n",
    "    MI_vector[j]   = -D/2*np.log(1-rho**2)\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "    for w in range(N_trials):  \n",
    "        optimizer      = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "        loss_history   = []\n",
    "        MIs            = []\n",
    "                              \n",
    "        model_mlp = tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(1,input_shape=[None,Map_D]),\n",
    "        ])  \n",
    "        seed=0   \n",
    "        for epoch in range(n_epochs):\n",
    "            seed=seed+1\n",
    "            Data_xy1_tensor, Data_xy1_shuffle_tensor = Data_generation_mi(rho, total_data_size, mean, cov, D, seed) \n",
    "            Mapper =  tf.contrib.kernel_methods.RandomFourierFeatureMapper(2*D,Map_D,name='Map')                 \n",
    "            Data_xy_map  = Mapper.map(Data_xy1_tensor)\n",
    "            Data_xy_shuffle_map = Mapper.map(Data_xy1_shuffle_tensor)\n",
    "            for sub_epoch in range(n_sub_epochs):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    T_xy = model_mlp(Data_xy_map)\n",
    "                    T_x_y = model_mlp(Data_xy_shuffle_map)\n",
    "                    loss_value = -(tf.reduce_mean(T_xy, axis=0) - tf.math.log(tf.reduce_mean(tf.math.exp(T_x_y)))) + l1*(tf.abs(tf.reduce_mean(T_xy, axis=0)))+l2*(tf.abs(tf.math.log(tf.reduce_mean(tf.math.exp(T_x_y), axis=0))))\n",
    "                \n",
    "                loss_history.append(loss_value.numpy())\n",
    "                grads = tape.gradient(loss_value, model_mlp.variables)\n",
    "                    \n",
    "                optimizer.apply_gradients(zip(grads, model_mlp.variables)) \n",
    "                MIs.append((tf.reduce_mean(T_xy, axis=0) - tf.math.log(tf.reduce_mean(tf.math.exp(T_x_y)))))\n",
    "        Nl = len(MIs)        \n",
    "        MI_vector_estimated1[j][w] = MIs[Nl-1].numpy()[0]\n",
    "        Bias1[j][w]                = MIs[Nl-1].numpy()[0] -MI_vector[j]\n",
    "        SE1[j][w]                  = (Bias1[j][w])**2 \n",
    "    Bias_mean1[j]                  = np.mean(Bias1[j])\n",
    "    RMSE_mean1[j]                  = np.sqrt(np.mean(SE1[j]))    \n",
    "    Variance1[j]                   = np.var(MI_vector_estimated1[j])    \n",
    "end = time.time()\n",
    "print (\"Total time for this block is \" + str(end-start) + \" seconds\") \n",
    "DataFrame_results2 = pd.DataFrame(np.array((Bias_mean1, RMSE_mean1, Variance1, rho_vector, MI_vector)).T,  columns = ['Bias', 'RMSE', 'Variance','Correlation', 'True Mutual Information'])    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Bias      RMSE  Variance  Correlation  True Mutual Information\n",
      "0 -0.010978  0.012168  0.000028          0.2                 0.020411\n",
      "1 -0.027839  0.034184  0.000393          0.5                 0.143841\n",
      "2 -0.051054  0.068270  0.002054          0.9                 0.830366\n"
     ]
    }
   ],
   "source": [
    "print (DataFrame_results2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
